


\chapter{Further Considerations} \label{chap:further}
We have now concluded the discussion of what has been implemented. This chapter will discuss what can be still done on the subject in the future. Such things can be improvements upon existing mechanisms, additional detection schemes, or even entirely new features.

\section{Adding Bad Behavior Detection}
We have already provided some examples of unusual MPTCP behavior that can be detected thanks to an MPTCP aware IDS. There are probably many more, and as the protocol matures and its use becomes more widespread, more and more people will have the incentive to attempt to exploit such behavior. At this point, the difficult decision comes from deciding whether or not a given action may lead to a potential attack. We could for example raise a notice if a host omits the MPTCP checksum after its usage was agreed upon (which results in the receiver closing the connection) but it is difficult to imagine this being of any use to an adversary. Other scenarios might prove more useful, but until real attacks exploiting a protocol weakness emerge, it is hard to tell which.

\section{Improving Events} 
As we mentioned in chapter \ref{chap:events}, the choice of implemented events was made to correspond clearly with the different option subtypes. There are certain MPTCP events which would be very useful instead of or in addition to those which have already been added. Adding multiple different events for a same option subtype would allow us to cut down on the parsing in script and make the scripts more modular. \\

Additionally, the current MPTCP events are quite independent from what is going on in the TCP flow they were generated in. There is an obvious use for events or data values which would bridge the gap. As an example, we attempted to log the TCP connections on which the initiator had attempted to use MPTCP, but the receiver was not able to do so. Our initial solution was to create a set of partial connections when encountering an MPTCP SYN packet. If we then observed an MPTCP SYN+ACK on the same subflow, we would determine MPTCP was correctly established and remove the entry from the table. If, when the connection started being used (or even when it terminated), the entry was still in the set, we would conclude that the connection did not receive any further MPTCP activity and that the use of the protocol had therefore been denied. However, this method relies on timing constraints that are not guaranteed by Bro. A script could receive the event for the removal of a connection before the SYN+ACK, and the notice would be raised before the entry had a chance to be deleted. \\

If we made MPTCP information more present within the regular TCP operation, operations like the one discussed would become much simpler. Indeed, if, for example, the connection record of a TCP flow contained information about the state of MPTCP, we could check on any event from that connection whether or not MPTCP was in use. An MPTCP SYN followed by a SYN+ACK where MPTCP is explicitly said to not be used would allowed us to unambiguously determine that the receiver (or a middle box along the path) has denied the use of the protocol. \\

Of course, this does not come for free. Adding MPTCP information into the connection record means adding more state in the Event Engine. Additionally, while the parsing of the option value is currently limited to when a user specifically asks for it, including MPTCP information in the connection state by default would mean parsing the options and their values for every TCP packet by default. Should MPTCP become commonplace, it may yet prove useful to do it anyway, and consider its presence as likely as DNS requests (which are also tracked by default).

\section{Recomputing Cryptographic Operations}
Our logging procedure so far is based on the addresses seen in the packet headers and those advertised by the hosts using the connection. Should the initial connection establishment be seen by Bro, we would technically have all the tools (specifically, keys of each host) to be able to compute all the cryptographic operations the hosts will need. We can, for example compute the 32-bit tokens used to identify MPTCP connections. With these tokens, we would be able to map joining subflows to their MPTCP connection more easily. Of course, given that Bro sees the tokens of many physical hosts and that the tokens are only required to be locally unique, there is a risk of collision. However, our current method of using the addresses suffers from the same drawback with the disadvantage of being more easily disturbed by middle boxes. \\

Recomputing the cryptographic operations would also allow the IDS to double check the authentication process when a subflow wishes to join an ongoing connection. Finally, keeping all the tokens in memory makes the system less reliant on a total view of the traffic. We using the addresses to match a flow to an MPTCP connection, we need to observe every single ADD ADDR sent by both hosts along any path they may use (some of which might not go through the IDS) to ensure all available addresses are known. By using the token, we need only observe the initial connection and we will be able to recognize joins from previously unknown addresses by the use of a known token. \\

Of course, all of this suffers from an obvious caveat: cryptographic operations are computationally expensive. This is even worse in the IDS because, while each host must compute the hashes and HMACs of its own connections, Bro would have to compute them for every single connection from every host whose traffic we can observer. Of course, we might try to find a middle ground such as computing all the tokens (a relatively simple hash), but not the HMACs. This may be quite profitable because, as we have just discussed, the tokens have more applications and are easier to compute. Checking the authentication during Joins is made somewhat redundant given that, in case of failure, we will simply see the connection close.


\section{Improving Join Flood Detection}
The solution we presented in chapter \ref{chap:script} was very basic. There are many ways to improve the detection of such an attack while, at the same time, reducing the load on the system. A first improvements is, as we discussed above, computing the connection tokens. An MP JOIN SYN packet with a wrong token will cause the receiver to send an rst. While a flood of wrong tokens is problematic, it is less so than a standard TCP SYN flood and will be detected as one anyway. What really interests us is MP JOIN SYN packets with correct tokens. These are the packet that will cause and additional load on the receiver by making it compute a HMAC. \\

Another big improvement is the possibility to install filters on hosts that are victim of or causing a flood attack. By observing the SYN flood script that was shipped with Bro v1.5 \cite{synflood}, we can see that the script actually samples the connection attempts, and once an attack has been detected, its starts dropping packets from the hosts involved. It keeps checking each victim from time to time (again, ignoring most packets) until the attack has subsided. This reduces the load on the system. As we saw, a large number of connection attempts of is worse than a much larger number of regular packets. \\

An issue with implementing this solution is that it is mostly oriented at real-time analysis. While that is the goal, most of the testing for this project was done using packet captures. The result is that, without needing to wait for the network to deliver packets, the Event Engine could consume packets from the capture as fast as the machine allows it. With a small network (or a very big machine), this make working off traces faster than on live captures. The Synflood script places a threshold for an attack at 15.000 SYNs within a minute (though it was written in 2007). In our test, we processed 10.000 SYNs in about two or three seconds. Therefore, without a huge packet capture to keep Bro busy, the execution on a trace will terminate before the system even thinks to check for a SYN flood attack.

\section{Internal Re-Assembly}
While observing the behavior of MPTCP itself is interesting and can potentially thwart certain threats, a large appeal of Deep Packet Inspection comes from the ability to examine the payload of packets. With the usage of MPTCP, this payload analysis is compromised because the data is fragmented over multiple seemingly unconnected TCP connections. Currently, when Bro will send packets through the analyzer tree, it is very likely that the application-layer analyzers will be unable to realize which protocols are in use because of missing segments. \\

Re-assembly could potentially be done in script with the events at our disposition, or at the most with a few modifications. We have already seen that we can determine which TCP subflows belong to the same MPTCP connection. Once we receive the data from all the involved subflows, the \texttt{mp\_dss} event provides us with the data sequence mapping necessary to put the data stream back together. It may become necessary to add the packet payload to the data sent with an \texttt{mp\_dss} event since, as we have seen above, there is currently no accurate way to link a TCP packet event with the DSS event that accompanies it. In the end however, this does not resolve the problem. Indeed, reconstructing the data stream in script might be interesting as a proof of concept, but the re-assembled stream can no longer be passed the the rest of the Even Engine's analyzers. \\

On the other hand, the Event Engine as it stands is not at all equipped to perform MPTCP re-assembly internally. This is where we see to what extent multipath connections break fundamental assumptions of modern networks. Within the Engine, the highest-level structure for representing a stream is a point-to-point connection. Streams are not expected to interact with each other, and each one is thus treated separately. Implementing internal re-assembly would require modifying Bro's architecture to a great extent. Before even attempting to understand the MPTCP data sequence mapping to re-assemble the stream, we would have to give Bro a way to make IP streams aware of each other. Analyzers would no longer work on a stream, but have to aggregate multiple streams before passing the data to their children. In addition, all the structures and mappings we have created to identify which MPTCP connection a TCP subflow belongs to would have to be done within the Event Engine. In the end, we would have to move a lot of the MPTCP comprehension from scripts into the Engine, which is originally designed to only output what it sees.
